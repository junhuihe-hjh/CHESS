{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057dca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import datasets\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import types\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n",
    "from transformers import Cache\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "import transformers.models.llama.modeling_llama as modeling_llama\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, eager_attention_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf95f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = r'/root/workspace/huggingface-models/Llama-3.1-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d477df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd34b7e7873b4196826faa6efcdbc9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=torch.bfloat16).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848e84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = r'/root/workspace/huggingface-datasets/c4'\n",
    "dataset = datasets.load_dataset(dataset_dir, data_files='en/c4-train.00000-of-01024.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c62967",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16\n",
    "device = 'cuda'\n",
    "\n",
    "ffn_size = model.config.intermediate_size\n",
    "num_layers = model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a21292",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_inputs_thresholds = [torch.zeros([1,], dtype=dtype, device=device) for _ in range(num_layers)]\n",
    "attention_outputs_thresholds = [torch.zeros([1,], dtype=dtype, device=device) for _ in range(num_layers)]\n",
    "\n",
    "gate_proj_states_thresholds_cats = [torch.zeros([1,], dtype=dtype, device=device) for _ in range(num_layers)]\n",
    "\n",
    "up_proj_states_abs_mean = [torch.zeros(ffn_size, dtype=dtype, device=device) for _ in range(num_layers)]\n",
    "importance_thresholds = [torch.zeros([1,], dtype=dtype, device=device) for _ in range(num_layers)]\n",
    "gate_proj_states_thresholds_chess = [torch.zeros(ffn_size, dtype=dtype, device=device) for _ in range(num_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3477ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = 0.5\n",
    "calibration_set_size = 64 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e08b6edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61eb6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile(x, q, dim=None):\n",
    "    if dim is None:\n",
    "        num_elements = x.numel()\n",
    "    else:\n",
    "        num_elements = 1\n",
    "        for dim_idx in range(len(x.shape)):\n",
    "            if dim_idx != dim:\n",
    "                num_elements *= x.shape[dim_idx]\n",
    "    \n",
    "    k = int(num_elements * q)\n",
    "\n",
    "    if dim is None:\n",
    "        return torch.kthvalue(x.view(-1), k, dim=-1).values\n",
    "    else:\n",
    "        return torch.kthvalue(x, k, dim=dim).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3974f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_attention_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    past_key_value: Optional[Cache] = None,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    **kwargs: Unpack[FlashAttentionKwargs],\n",
    ") -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "    global layer_idx\n",
    "    \n",
    "    input_shape = hidden_states.shape[:-1]\n",
    "    hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "    layer_idx = self.layer_idx\n",
    "    attention_inputs_thresholds[layer_idx] += quantile(hidden_states.abs(), q=sparsity)\n",
    "\n",
    "    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "    cos, sin = position_embeddings\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "    attention_interface: Callable = eager_attention_forward\n",
    "    if self.config._attn_implementation != \"eager\":\n",
    "        if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
    "            logger.warning_once(\n",
    "                \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
    "                'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
    "            )\n",
    "        else:\n",
    "            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "\n",
    "    attn_output, attn_weights = attention_interface(\n",
    "        self,\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attention_mask,\n",
    "        dropout=0.0 if not self.training else self.attention_dropout,\n",
    "        scaling=self.scaling,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "\n",
    "    attention_outputs_thresholds[layer_idx] += quantile(attn_output.abs(), q=sparsity)\n",
    "\n",
    "    attn_output = self.o_proj(attn_output)\n",
    "    return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a9ac675",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.model.layers:\n",
    "    layer.self_attn.forward = types.MethodType(llama_attention_forward, layer.self_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6571be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_mlp_forward(self, x):\n",
    "    gate_proj_states = self.act_fn(self.gate_proj(x))\n",
    "    up_proj_states = self.up_proj(x)\n",
    "    down_proj = self.down_proj(gate_proj_states * up_proj_states)\n",
    "\n",
    "    ffn_size = self.config.intermediate_size\n",
    "\n",
    "    gate_proj_states_thresholds_cats[layer_idx] += quantile(gate_proj_states.abs(), q=sparsity)\n",
    "    up_proj_states_abs_mean[layer_idx] += up_proj_states.abs().view(-1, ffn_size).mean(dim=0)\n",
    "\n",
    "    return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cbf2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.model.layers:\n",
    "    layer.mlp.forward = types.MethodType(llama_mlp_forward, layer.mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a200247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65738it [02:02, 536.68it/s]                           \n"
     ]
    }
   ],
   "source": [
    "max_len = 64 * 1024\n",
    "num_processed_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=calibration_set_size) as pbar:\n",
    "        for sample_idx in range(len(dataset['train'])):\n",
    "            input_ids = tokenizer.encode(dataset[\"train\"][sample_idx]['text'])\n",
    "            input_ids = input_ids[:max_len]\n",
    "\n",
    "            model(torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device))\n",
    "            \n",
    "            pbar.update(len(input_ids))\n",
    "            \n",
    "            num_processed_tokens += len(input_ids)\n",
    "            if num_processed_tokens >= calibration_set_size:\n",
    "                break\n",
    "\n",
    "num_samples = sample_idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "979e2d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(num_layers):\n",
    "    attention_inputs_thresholds[layer_idx] /= num_samples\n",
    "    attention_outputs_thresholds[layer_idx] /= num_samples\n",
    "    gate_proj_states_thresholds_cats[layer_idx] /= num_samples\n",
    "    up_proj_states_abs_mean[layer_idx] /= num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "541a45af",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "484fc83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.model.layers:\n",
    "    layer.self_attn.forward = types.MethodType(modeling_llama.LlamaAttention.forward, layer.self_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dfa7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_mlp_forward(self, x):\n",
    "    global layer_idx\n",
    "    \n",
    "    gate_proj_states = self.act_fn(self.gate_proj(x))\n",
    "    up_proj_states = self.up_proj(x)\n",
    "    down_proj = self.down_proj(gate_proj_states * up_proj_states)\n",
    "\n",
    "    importance_scores = gate_proj_states.view(-1, ffn_size).abs() * up_proj_states_abs_mean[layer_idx].to(device)\n",
    "    importance_thresholds[layer_idx] += quantile(importance_scores, q=sparsity)\n",
    "    layer_idx = (layer_idx + 1) % num_layers\n",
    "\n",
    "    return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5599eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.model.layers:\n",
    "    layer.mlp.forward = types.MethodType(llama_mlp_forward, layer.mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1221dfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65738it [01:22, 796.08it/s]                            \n"
     ]
    }
   ],
   "source": [
    "max_len = 64 * 1024\n",
    "num_processed_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=calibration_set_size) as pbar:\n",
    "        for sample_idx in range(len(dataset['train'])):\n",
    "            input_ids = tokenizer.encode(dataset[\"train\"][sample_idx]['text'])\n",
    "            input_ids = input_ids[:max_len]\n",
    "\n",
    "            model(torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device))\n",
    "            \n",
    "            pbar.update(len(input_ids))\n",
    "            \n",
    "            num_processed_tokens += len(input_ids)\n",
    "            if num_processed_tokens >= calibration_set_size:\n",
    "                break\n",
    "\n",
    "num_samples = sample_idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "971e8a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(num_layers):\n",
    "    importance_thresholds[layer_idx] /= num_samples\n",
    "    gate_proj_states_thresholds_chess[layer_idx] = importance_thresholds[layer_idx] / up_proj_states_abs_mean[layer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47107cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = r'../thresholds'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'attention_inputs_thresholds': attention_inputs_thresholds,\n",
    "    'attention_outputs_thresholds': attention_outputs_thresholds,\n",
    "    'gate_proj_states_thresholds_cats': gate_proj_states_thresholds_cats,\n",
    "    'gate_proj_states_thresholds_chess': gate_proj_states_thresholds_chess,\n",
    "}, f'{output_dir}/0_5.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CHESS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
